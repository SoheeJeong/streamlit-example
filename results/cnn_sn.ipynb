{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    enfj\n",
       "1    enfj\n",
       "2    enfj\n",
       "3    enfj\n",
       "4    enfj\n",
       "Name: MBTI, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_df = pd.read_csv('crop128/all.csv')\n",
    "\n",
    "photo_df = pd.DataFrame(columns=['photo'], dtype= 'object')\n",
    "\n",
    "# put the photo data into a dataframe\n",
    "for i in range(len(mbti_df)):\n",
    "    image = transform(Image.open('crop128/' + mbti_df['filename'][i]))\n",
    "    photo_df.loc[i] = [image]\n",
    "\n",
    "mbti_df = mbti_df['MBTI']\n",
    "\n",
    "mbti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_df.iloc[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: MBTI, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = ['s', 'n']\n",
    "\n",
    "# if mbti_df['MBTI'] includes alphabet[0], then mbti_df['MBTI'] = 1, else 0\n",
    "mbti_df = mbti_df.apply(lambda x: 1 if alphabet[0] in x else 0)\n",
    "\n",
    "mbti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1589\n",
      "1    1455\n",
      "Name: MBTI, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print the statistics of the mbti_df\n",
    "print(mbti_df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data includes the element of 2-dim tensor\n",
    "train_data = photo_df['photo'].values\n",
    "train_label = mbti_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 data 중 train의 비율\n",
    "train_ratio = 0.8\n",
    "\n",
    "train_idx = np.random.choice(len(train_data), int(len(train_data) * train_ratio), replace=False)\n",
    "test_idx = np.array(list(set(range(len(train_data))) - set(train_idx)))\n",
    "\n",
    "test_data = train_data[test_idx]\n",
    "test_label = train_label[test_idx]\n",
    "\n",
    "train_data = train_data[train_idx]\n",
    "train_label = train_label[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MBTI_Dataset(Dataset):\n",
    "    def __init__(self, train_label, train_data):\n",
    "        self.train_label = train_label\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        mbti = self.train_label[idx]\n",
    "        photo = self.train_data[idx]\n",
    "\n",
    "        return mbti, photo\n",
    "\n",
    "# parameter 값은 이것을 변경해주세요\n",
    "in_ch1 = 1\n",
    "out_ch1 = 8\n",
    "ker1 = 6\n",
    "stride1 = 1\n",
    "pad1 = 0\n",
    "\n",
    "out_ch2 = 16\n",
    "ker2 = 4\n",
    "stride2 = 1\n",
    "pad2 = 0\n",
    "\n",
    "out_ch3 = 32\n",
    "ker3 = 2\n",
    "stride3 = 1\n",
    "pad3 = 0\n",
    "\n",
    "out_ch4 = 64\n",
    "ker4 = 2\n",
    "stride4 = 1\n",
    "pad4 = 0\n",
    "\n",
    "out_ch5 = 128\n",
    "ker5 = 2\n",
    "stride5 = 1\n",
    "pad5 = 0\n",
    "\n",
    "\n",
    "pool_size1 = 2\n",
    "pool_size2 = 2\n",
    "pool_size3 = 2\n",
    "pool_size4 = 2\n",
    "pool_size5 = 2\n",
    "\n",
    "\n",
    "out_feat1 = 128\n",
    "out_feat2 = 64\n",
    "out_feat3 = 16\n",
    "out_feat4 = 1\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Net, self).__init__()\n",
    "        input_height, input_width = input_shape\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_ch1, out_channels = out_ch1, kernel_size = ker1, stride = stride1, padding = pad1)\n",
    "        self.pool1 = nn.MaxPool2d(pool_size1, pool_size1)\n",
    "\n",
    "        output1_height, output1_width = (input_height - ker1 + 2 * pad1) / stride1 + 1, (input_width - ker1 + 2 * pad1) / stride1 + 1\n",
    "        output1_height, output1_width = int(output1_height / pool_size1), int(output1_width / pool_size1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = out_ch1, out_channels = out_ch2, kernel_size = ker2, stride = stride2, padding = pad2)\n",
    "        self.pool2 = nn.MaxPool2d(pool_size2, pool_size2)\n",
    "\n",
    "        output2_height, output2_width = (output1_height - ker2 + 2 * pad2) / stride2 + 1, (output1_width - ker2 + 2 * pad2) / stride2 + 1\n",
    "        output2_height, output2_width = int(output2_height / pool_size2), int(output2_width / pool_size2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = out_ch2, out_channels = out_ch3, kernel_size = ker3, stride = stride3, padding = pad3)\n",
    "        self.pool3 = nn.MaxPool2d(pool_size3, pool_size3)\n",
    "\n",
    "        output3_height, output3_width = (output2_height - ker3 + 2 * pad3) / stride3 + 1, (output2_width - ker3 + 2 * pad3) / stride3 + 1\n",
    "        output3_height, output3_width = int(output3_height / pool_size3), int(output3_width / pool_size3)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels = out_ch3, out_channels = out_ch4, kernel_size = ker4, stride = stride4, padding = pad4)\n",
    "        self.pool4 = nn.MaxPool2d(pool_size4, pool_size4)\n",
    "\n",
    "        output4_height, output4_width = (output3_height - ker4 + 2 * pad4) / stride4 + 1, (output3_width - ker4 + 2 * pad4) / stride4 + 1\n",
    "        output4_height, output4_width = int(output4_height / pool_size4), int(output4_width / pool_size4)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_ch4 * output4_height * output4_width, out_feat1)\n",
    "        self.fc2 = nn.Linear(out_feat1, out_feat2)\n",
    "        self.fc3 = nn.Linear(out_feat2, out_feat3)\n",
    "        self.fc4 = nn.Linear(out_feat3, out_feat4)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        #print('1')\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        #print('2')\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        #print('3')\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        #print('4')\n",
    "        #x = self.pool5(F.relu(self.conv5(x)))\n",
    "        #print('5')\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # 이 부분은 변경하셔도 괜찮아요. relu로 할지 sigmoid로 할지\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "class cnn_model():\n",
    "    def __init__(self, model, lr=0.01, epochs=100, momentum = 0.6):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.momentum = momentum\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum = self.momentum)\n",
    "    \n",
    "    def fit(self, X_train, y_train):        \n",
    "        self.trainloader = DataLoader(MBTI_Dataset(X_train, y_train), batch_size=64, shuffle=False)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for i, data in enumerate(self.trainloader):\n",
    "                inputs, labels = data\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                labels.unsqueeze_(1)\n",
    "                loss = self.criterion(outputs, labels.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x.unsqueeze(0))\n",
    "        return y_pred\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'lr': self.lr, 'epochs': self.epochs, 'momentum': self.momentum}\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net((train_data[0].shape[1], train_data[0].shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cross_val_score(model, train_data, label, cv=5):\n",
    "    k = cv\n",
    "    kf = KFold(n_splits=k, random_state=42, shuffle=True)\n",
    "\n",
    "    acc_score = []\n",
    "    auc_score = []\n",
    "    \n",
    "    for train_index , test_index in kf.split(train_data):\n",
    "        X_train , X_test = train_data[train_index],train_data[test_index]\n",
    "        y_train , y_test = label[train_index] , label[test_index]\n",
    "        \n",
    "        if(np.unique(y_test).shape[0] == 1):\n",
    "            print('only one class')\n",
    "            continue\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        pred_values = []\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            pred = model.predict(X_test[i])\n",
    "            pred_values.append(pred.item())\n",
    "\n",
    "        auc = roc_auc_score(y_test, pred_values)\n",
    "        auc_score.append(auc)\n",
    "        \n",
    "    avg_acc_score = sum(acc_score)/k\n",
    "    avg_auc_score = sum(auc_score)/k\n",
    "    \n",
    "    return avg_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tupes of (lr, momentum, epochs) randomly\n",
    "# random cv를 몇번 돌릴 것인지...\n",
    "random_num = 50\n",
    "\n",
    "\n",
    "# parameter 값이 이 범위 내에서 나옵니다\n",
    "lrs = np.linspace(0.01, 0.06, 30)\n",
    "momentums = np.linspace(0.0, 0.9, 20)\n",
    "epochss = np.linspace(150, 400, 25, dtype=int)\n",
    "\n",
    "params = [(lr, momentum, epochs) for lr in lrs for momentum in momentums for epochs in epochss]\n",
    "np.random.shuffle(params)\n",
    "params = params[:random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improved_accuracy: 0.0\n",
      "learning rate: 0.02896551724137931 momentum: 0.28421052631578947 epochs: 400\n",
      "improved_accuracy: -0.3284072249589487\n",
      "learning rate: 0.04448275862068965 momentum: 0.04736842105263158 epochs: 327\n",
      "improved_accuracy: -0.16420361247947435\n",
      "learning rate: 0.022068965517241378 momentum: 0.14210526315789473 epochs: 191\n",
      "improved_accuracy: -0.16420361247947435\n",
      "learning rate: 0.05655172413793103 momentum: 0.4736842105263158 epochs: 389\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.013448275862068966 momentum: 0.7105263157894737 epochs: 400\n",
      "-------improved!--------\n",
      "best threshold:  0.7104999999998116\n",
      "best accuracy:  0.5254515599343186\n",
      "best improved accuracy:  0.16420361247947435\n",
      "best auc:  0.5103123986596044\n",
      "best mean:  0.0015387092102166014\n",
      "best var:  6.226353457870543e-15\n",
      "best learning rate:  0.013448275862068966 best momentum:  0.7105263157894737 best epochs:  400\n",
      "------------------------\n",
      "improved_accuracy: -0.49261083743842304\n",
      "learning rate: 0.022068965517241378 momentum: 0.0 epochs: 243\n",
      "improved_accuracy: -0.16420361247947435\n",
      "learning rate: 0.039310344827586205 momentum: 0.5210526315789474 epochs: 275\n",
      "improved_accuracy: -0.3284072249589487\n",
      "learning rate: 0.04275862068965517 momentum: 0.0 epochs: 191\n",
      "improved_accuracy: 0.0\n",
      "learning rate: 0.03586206896551724 momentum: 0.04736842105263158 epochs: 368\n",
      "improved_accuracy: -0.3284072249589487\n",
      "learning rate: 0.02379310344827586 momentum: 0.4263157894736842 epochs: 379\n",
      "improved_accuracy: -0.16420361247947435\n",
      "learning rate: 0.05827586206896551 momentum: 0.14210526315789473 epochs: 160\n",
      "improved_accuracy: 0.0\n",
      "learning rate: 0.02896551724137931 momentum: 0.14210526315789473 epochs: 264\n",
      "improved_accuracy: 0.3284072249589487\n",
      "learning rate: 0.05310344827586207 momentum: 0.4736842105263158 epochs: 264\n",
      "-------improved!--------\n",
      "best threshold:  0.7212999999998104\n",
      "best accuracy:  0.5270935960591133\n",
      "best improved accuracy:  0.3284072249589487\n",
      "best auc:  0.5106204734623284\n",
      "best mean:  0.0006881127719627789\n",
      "best var:  1.4548507905024981e-15\n",
      "best learning rate:  0.05310344827586207 best momentum:  0.4736842105263158 best epochs:  264\n",
      "------------------------\n",
      "improved_accuracy: -0.16420361247947435\n",
      "learning rate: 0.05655172413793103 momentum: 0.4263157894736842 epochs: 233\n",
      "improved_accuracy: 0.0\n",
      "learning rate: 0.01 momentum: 0.7578947368421053 epochs: 160\n",
      "improved_accuracy: 0.3284072249589487\n",
      "learning rate: 0.05310344827586207 momentum: 0.2368421052631579 epochs: 160\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.05137931034482758 momentum: 0.5210526315789474 epochs: 243\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.054827586206896546 momentum: 0.14210526315789473 epochs: 306\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.039310344827586205 momentum: 0.6157894736842106 epochs: 160\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.01 momentum: 0.6157894736842106 epochs: 170\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.03758620689655172 momentum: 0.04736842105263158 epochs: 181\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.03758620689655172 momentum: 0.5210526315789474 epochs: 295\n",
      "improved_accuracy: 0.3284072249589487\n",
      "learning rate: 0.03758620689655172 momentum: 0.6157894736842106 epochs: 222\n",
      "improved_accuracy: 0.6568144499178974\n",
      "learning rate: 0.01689655172413793 momentum: 0.28421052631578947 epochs: 191\n",
      "-------improved!--------\n",
      "best threshold:  0.7409999999998083\n",
      "best accuracy:  0.5303776683087028\n",
      "best improved accuracy:  0.6568144499178974\n",
      "best auc:  0.5101070154577884\n",
      "best mean:  0.0003726313971853155\n",
      "best var:  4.695636660473933e-16\n",
      "best learning rate:  0.01689655172413793 best momentum:  0.28421052631578947 best epochs:  191\n",
      "------------------------\n",
      "improved_accuracy: 0.3284072249589487\n",
      "learning rate: 0.03413793103448275 momentum: 0.8052631578947369 epochs: 254\n",
      "improved_accuracy: 0.49261083743842304\n",
      "learning rate: 0.027241379310344822 momentum: 0.5684210526315789 epochs: 358\n",
      "improved_accuracy: 0.0\n",
      "learning rate: 0.05310344827586207 momentum: 0.8526315789473684 epochs: 379\n",
      "improved_accuracy: 0.3284072249589487\n",
      "learning rate: 0.01 momentum: 0.4263157894736842 epochs: 379\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.046206896551724136 momentum: 0.2368421052631579 epochs: 285\n",
      "improved_accuracy: 0.0\n",
      "learning rate: 0.06 momentum: 0.7105263157894737 epochs: 295\n",
      "improved_accuracy: 0.3284072249589487\n",
      "learning rate: 0.041034482758620684 momentum: 0.9 epochs: 285\n",
      "improved_accuracy: 0.16420361247947435\n",
      "learning rate: 0.046206896551724136 momentum: 0.2368421052631579 epochs: 368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [134], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m best_model \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[1;32m---> 17\u001b[0m     model\u001b[39m.\u001b[39;49mfit(train_data, train_label)\n\u001b[0;32m     19\u001b[0m     train_pred_values \u001b[39m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_data)):\n",
      "Cell \u001b[1;32mIn [130], line 141\u001b[0m, in \u001b[0;36mcnn_model.fit\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m    139\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(outputs, labels\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m    140\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> 141\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[1;32mc:\\Users\\opush\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\opush\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\opush\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\optim\\sgd.py:146\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m             momentum_buffer_list\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmomentum_buffer\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 146\u001b[0m sgd(params_with_grad,\n\u001b[0;32m    147\u001b[0m     d_p_list,\n\u001b[0;32m    148\u001b[0m     momentum_buffer_list,\n\u001b[0;32m    149\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    150\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    151\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[0;32m    156\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    158\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[1;32mc:\\Users\\opush\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\optim\\sgd.py:197\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 197\u001b[0m func(params,\n\u001b[0;32m    198\u001b[0m      d_p_list,\n\u001b[0;32m    199\u001b[0m      momentum_buffer_list,\n\u001b[0;32m    200\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    201\u001b[0m      momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[0;32m    202\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    203\u001b[0m      dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[0;32m    204\u001b[0m      nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[0;32m    205\u001b[0m      has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[0;32m    206\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize)\n",
      "File \u001b[1;32mc:\\Users\\opush\\anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\optim\\sgd.py:233\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    231\u001b[0m     momentum_buffer_list[i] \u001b[39m=\u001b[39m buf\n\u001b[0;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m     buf\u001b[39m.\u001b[39;49mmul_(momentum)\u001b[39m.\u001b[39madd_(d_p, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m dampening)\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m nesterov:\n\u001b[0;32m    236\u001b[0m     d_p \u001b[39m=\u001b[39m d_p\u001b[39m.\u001b[39madd(buf, alpha\u001b[39m=\u001b[39mmomentum)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import roc curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "models = [cnn_model(net, lr, epochs, momentum) for lr, momentum, epochs in params]\n",
    "\n",
    "overall_best_threshold = 0\n",
    "overall_best_accuracy = 0\n",
    "overall_best_improved_accuracy = 0\n",
    "overall_best_auc = 0\n",
    "overall_best_mean = 0\n",
    "overall_best_var = 0\n",
    "overall_best_params = None\n",
    "\n",
    "best_model = None\n",
    "\n",
    "for model in models:\n",
    "    model.fit(train_data, train_label)\n",
    "\n",
    "    train_pred_values = []\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        pred = model.predict(train_data[i])\n",
    "        train_pred_values.append(pred.item())\n",
    "\n",
    "    #calculate mean and variance of train_pred\n",
    "    train_pred_values = np.array(train_pred_values)\n",
    "    train_pred_mean = np.mean(train_pred_values)\n",
    "    train_pred_var = np.var(train_pred_values)\n",
    "\n",
    "    #normalize train_pred_values\n",
    "    train_pred_values = (train_pred_values - train_pred_mean) / np.sqrt(train_pred_var)\n",
    "\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for threshold in np.arange(-1, 1, 0.0001):\n",
    "        y_pred = np.array(train_pred_values) > threshold\n",
    "        score = accuracy_score(train_label, y_pred)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "\n",
    "    pred_values = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        pred = model.predict(test_data[i])\n",
    "        pred_values.append(pred.item())\n",
    "\n",
    "    #quantize predictions\n",
    "    pred_values = np.array(pred_values)\n",
    "\n",
    "    #normalize pred_values\n",
    "    pred_values = (pred_values - train_pred_mean) / np.sqrt(train_pred_var)\n",
    "    guess3 = roc_auc_score(test_label, pred_values)\n",
    "\n",
    "    pred_values[pred_values >= best_threshold] = 1\n",
    "    pred_values[pred_values < best_threshold] = 0\n",
    "\n",
    "    #calculate accuracy score\n",
    "\n",
    "    guess1 = accuracy_score(test_label, pred_values)\n",
    "\n",
    "    # random guess\n",
    "\n",
    "    pred_values = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        pred = 0\n",
    "        pred_values.append(pred)\n",
    "\n",
    "    #quantize predictions\n",
    "    pred_values = np.array(pred_values)\n",
    "\n",
    "    #calculate accuracy score\n",
    "    guess2_1 = accuracy_score(test_label, pred_values)\n",
    "    \n",
    "    pred_values = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        pred = 1\n",
    "        pred_values.append(pred)\n",
    "    \n",
    "    #quantize predictions\n",
    "    pred_values = np.array(pred_values)\n",
    "\n",
    "    #calculate accuracy score\n",
    "    guess2_2 = accuracy_score(test_label, pred_values)\n",
    "\n",
    "    guess2 = max(guess2_1, guess2_2)\n",
    "\n",
    "    improved_accuracy = (guess1 - guess2) * 100\n",
    "\n",
    "    print('improved_accuracy:', improved_accuracy)\n",
    "    print('learning rate:', model.get_params()['lr'], 'momentum:', model.get_params()['momentum'], 'epochs:', model.get_params()['epochs'])\n",
    "\n",
    "    if(overall_best_improved_accuracy < improved_accuracy):\n",
    "        overall_best_threshold = best_threshold\n",
    "        overall_best_accuracy = guess1\n",
    "        overall_best_improved_accuracy = improved_accuracy\n",
    "        overall_best_auc = guess3\n",
    "        overall_best_mean = train_pred_mean\n",
    "        overall_best_var = train_pred_var\n",
    "        overall_best_params = model.get_params()\n",
    "        best_model = model\n",
    "\n",
    "        print('-------improved!--------')\n",
    "        print('best threshold: ', best_threshold)\n",
    "        print('best accuracy: ', guess1)\n",
    "        print('best improved accuracy: ', improved_accuracy)\n",
    "        print('best auc: ', guess3)\n",
    "        print('best mean: ', train_pred_mean)\n",
    "        print('best var: ', train_pred_var)\n",
    "        print('best learning rate: ', overall_best_params['lr'], 'best momentum: ', overall_best_params['momentum'], 'best epochs: ', overall_best_params['epochs'])\n",
    "        print('------------------------')\n",
    "\n",
    "if(overall_best_params == None):\n",
    "    print('test failed!')\n",
    "else:\n",
    "    print('-------------final result-------------')\n",
    "    print('best threshold: ', overall_best_accuracy)\n",
    "    print('best accuracy: ', overall_best_accuracy)\n",
    "    print('best improved accuracy: ', overall_best_improved_accuracy)\n",
    "    print('best auc: ', overall_best_auc)   \n",
    "    print('best_mean: ', overall_best_mean)\n",
    "    print('best_var: ', overall_best_var)\n",
    "    print('best learning rate: ', overall_best_params['lr'], 'best momentum: ', overall_best_params['momentum'], 'best epochs: ', overall_best_params['epochs'])\n",
    "    best_model.save('cnn_sn.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------final result-------------\n",
      "best threshold:  0.5303776683087028\n",
      "best accuracy:  0.5303776683087028\n",
      "best improved accuracy:  0.6568144499178974\n",
      "best auc:  0.5101070154577884\n",
      "best_mean:  0.0003726313971853155\n",
      "best_var:  4.695636660473933e-16\n",
      "best learning rate:  0.01689655172413793 best momentum:  0.28421052631578947 best epochs:  191\n"
     ]
    }
   ],
   "source": [
    "if(overall_best_params == None):\n",
    "    print('test failed!')\n",
    "else:\n",
    "    print('-------------final result-------------')\n",
    "    print('best threshold: ', overall_best_accuracy)\n",
    "    print('best accuracy: ', overall_best_accuracy)\n",
    "    print('best improved accuracy: ', overall_best_improved_accuracy)\n",
    "    print('best auc: ', overall_best_auc)   \n",
    "    print('best_mean: ', overall_best_mean)\n",
    "    print('best_var: ', overall_best_var)\n",
    "    print('best learning rate: ', overall_best_params['lr'], 'best momentum: ', overall_best_params['momentum'], 'best epochs: ', overall_best_params['epochs'])\n",
    "    best_model.save('cnn_sn.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('PyTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e6deea03f373b4c861bc3d562619dfb3a773dcf49daa365d4720af764a85b00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
